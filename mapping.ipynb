{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Sebastian Szyller, Buse Gul Atli\n",
    "Copyright 2020 Secure Systems Group, Aalto University, https://ssg.aalto.fi\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains experiment with the mapping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:42.891732Z",
     "start_time": "2020-08-26T08:48:42.863492Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:44.300563Z",
     "start_time": "2020-08-26T08:48:43.075549Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitstring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/1554519498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwatermark_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dawn-dynamic-adversarial-watermarking-of-neural-networks/filter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbitstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitstring'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import copy\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.utils.data as data\n",
    "import torchvision as tv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import config_helper\n",
    "import filter as watermark_filter\n",
    "import logger\n",
    "import models\n",
    "import score\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "log = logger.Logger(prefix=\">>>\")\n",
    "\n",
    "class SimpleDataset(data.Dataset):\n",
    "    def __init__(self, dataset: List[Tuple[Any, int]]) -> None:\n",
    "        self.data, self.labels = zip(*dataset)\n",
    "        self.count = len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int) -> (Any, int):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:44.339706Z",
     "start_time": "2020-08-26T08:48:44.303206Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data(dataset_name: str, victim_data_path: str, input_size: int) -> (data.Dataset, data.Dataset):\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]\n",
    "\n",
    "    if dataset_name == \"MNIST\":\n",
    "        dataset = tv.datasets.MNIST\n",
    "        transformations = tv.transforms.Compose([\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    else:\n",
    "        log.error(\"MNIST is the only supported datasets at the moment. Throwing...\")\n",
    "        raise ValueError(dataset_name)\n",
    "\n",
    "    train_set = dataset(victim_data_path, train=True, transform=transformations, download=True)\n",
    "    test_set = dataset(victim_data_path, train=False, transform=transformations, download=True)\n",
    "    \n",
    "    log.info(\"Training ({}) samples: {}\\nTest samples: {}\\nSaved in: {}\".format(dataset_name, len(train_set), len(test_set), victim_data_path))\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def setup_victim_model(model_architecture: str, model_path: str, number_of_classes: int) -> nn.Module:\n",
    "    available_models = {\n",
    "        \"MNIST_L5\": models.MNIST_L5_with_latent,\n",
    "    }\n",
    "\n",
    "    model = available_models[model_architecture]()\n",
    "\n",
    "    if model is None:\n",
    "        log.error(\"Incorrect model architecture specified or architecture not available.\")\n",
    "        raise ValueError(model_architecture)\n",
    "\n",
    "    models.load_state(model, model_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_file(file_path: str) -> List[Tuple]:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def test_model(model: nn.Module, test_set: data.DataLoader, number_of_classes: int) -> (score.FloatScore, score.DictScore):\n",
    "    \"\"\"Test the model on the test dataset.\"\"\"\n",
    "    # model.eval is used for ImageNet models, batchnorm or dropout layers will work in eval mode.\n",
    "    model.eval()\n",
    "\n",
    "    def test_average() -> score.FloatScore:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for (inputs, yreal) in tqdm(test_set, unit=\"images\", desc=\"Testing model (average)\", leave=True, ascii=True):\n",
    "                inputs, yreal = inputs.cuda(), yreal.cuda()\n",
    "\n",
    "                ypred, _ = model(inputs)\n",
    "                _, predicted = torch.max(ypred.data, 1)\n",
    "\n",
    "                total += yreal.size(0)\n",
    "                correct += (predicted == yreal).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        log.info(\"Accuracy of the network on the {} test images (average): {}\".format(total, accuracy))\n",
    "        with open('epoch_logs.txt', 'a+') as file:\n",
    "            file.write('Test Acc: {}\\n'.format(accuracy))\n",
    "        return score.FloatScore(accuracy)\n",
    "\n",
    "    def test_per_class() -> score.DictScore:\n",
    "        class_correct = list(0. for _ in range(number_of_classes))\n",
    "        class_total = list(0. for _ in range(number_of_classes))\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (inputs, yreal) in tqdm(test_set, unit=\"images\", desc=\"Testing model (per class)\", leave=True, ascii=True):\n",
    "                inputs, yreal = inputs.cuda(), yreal.cuda()\n",
    "\n",
    "                total += yreal.size(0)\n",
    "\n",
    "                ypred, _ = model(inputs)\n",
    "                _, predicted = torch.max(ypred, 1)\n",
    "                c = (predicted == yreal).squeeze()\n",
    "                for i in range(yreal.shape[0]):\n",
    "                    label = yreal[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        log.info(\"Accuracy of the network on the {} test images (per-class):\".format(total))\n",
    "\n",
    "        per_class_accuracy = {}\n",
    "        for i in range(number_of_classes):\n",
    "            accuracy = 100 * class_correct[i] / (class_total[i] + 0.0001)\n",
    "            per_class_accuracy[i] = accuracy\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                i, accuracy))\n",
    "\n",
    "        return score.DictScore(per_class_accuracy)\n",
    "\n",
    "    return test_average(), test_per_class()\n",
    "\n",
    "\n",
    "def get_shapes(model: nn.Module, test_set: data.DataLoader) -> (torch.Size, List[torch.Size]):\n",
    "    \"\"\"Returns input and latent sizes.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for (inputs, yreal) in test_set:\n",
    "            inputs, yreal = inputs.cuda(), yreal.cuda()\n",
    "\n",
    "            ypred, latents = model(inputs)\n",
    "            watermark_shape = inputs[0].cpu().shape\n",
    "            latents_shapes = [torch.Size([l.cpu().shape[1]]) for l in latents]\n",
    "            break\n",
    "\n",
    "    return watermark_shape, latents_shapes\n",
    "\n",
    "\n",
    "def compare_distributions(\n",
    "    model: nn.Module, test_set: data.DataLoader,\n",
    "    wf: watermark_filter.WatermarkFilter,\n",
    "    wf_latents: List[watermark_filter.WatermarkFilter]) -> List[List]:\n",
    "    \n",
    "    with_wm_orig = 0\n",
    "    without_wm_orig = 0\n",
    "\n",
    "    latent_n = len(wf_latents)\n",
    "    latent_batches = [[] for _ in range(latent_n)]\n",
    "    with_without = [\n",
    "    {\n",
    "        \"with_wm_latent\": 0,\n",
    "        \"without_wm_latent\": 0\n",
    "    } \n",
    "    for _ in range(latent_n)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, _) in tqdm(test_set, unit=\"images\", desc=\"Watermark Filter\", leave=True, ascii=True):\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "            model.eval()\n",
    "            _, latents = model(inputs)\n",
    "            inputs = inputs.cpu()\n",
    "\n",
    "\n",
    "            for x in inputs:\n",
    "                if wf.is_watermark(x):\n",
    "                    with_wm_orig += 1\n",
    "                else:\n",
    "                    without_wm_orig += 1\n",
    "\n",
    "            for i in range(latent_n):\n",
    "                lat_repr = latents[i].cpu()\n",
    "                latent_batches[i].append(lat_repr)\n",
    "                \n",
    "                for x in lat_repr:\n",
    "                    if wf_latents[i].is_watermark(x):\n",
    "                        with_without[i][\"with_wm_latent\"] += 1\n",
    "                    else:\n",
    "                        with_without[i][\"without_wm_latent\"] += 1\n",
    "\n",
    "    log.info(\"Watermarked: {}\".format(with_wm_orig))\n",
    "    log.info(\"Not watermarked: {}\".format(without_wm_orig))\n",
    "    log.info(\"Ratio: {}\".format(with_wm_orig * 100 / without_wm_orig))\n",
    "\n",
    "    for i in range(latent_n):\n",
    "        log.info(\"Watermarked latent: {}\".format(with_without[i][\"with_wm_latent\"]))\n",
    "        log.info(\"Not watermarked latent: {}\".format(with_without[i][\"without_wm_latent\"]))\n",
    "        log.info(\"Ratio latent: {}\".format(with_without[i][\"with_wm_latent\"] * 100 / with_without[i][\"without_wm_latent\"]))\n",
    "\n",
    "    \n",
    "    return latent_batches\n",
    "\n",
    "\n",
    "def perturb(img, e, min_pixel=-1., max_pixel=1.):\n",
    "    r = max_pixel - min_pixel\n",
    "    b = r * torch.rand(img.shape)\n",
    "    b += min_pixel\n",
    "    noise = e * b\n",
    "    noise = noise.cuda()\n",
    "\n",
    "    return torch.clamp(img + noise, min_pixel, max_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:44.356422Z",
     "start_time": "2020-08-26T08:48:44.341365Z"
    }
   },
   "outputs": [],
   "source": [
    "config = config_helper.load_config(\"configurations/mapping/mapping-mnist-l5.ini\")\n",
    "\n",
    "victim_path = \"data/models/victim_mnist_l5.pt\"\n",
    "\n",
    "config_helper.print_config(config)\n",
    "log.info(\"Victim model path: {}.\".format(victim_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:47.330937Z",
     "start_time": "2020-08-26T08:48:44.357595Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_victim_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/2186675878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  Setup model architecture and load models from file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_victim = setup_victim_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DEFAULT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_architecture\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvictim_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     int(config[\"DEFAULT\"][\"number_of_classes\"]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_victim_model' is not defined"
     ]
    }
   ],
   "source": [
    "#  Setup model architecture and load models from file.\n",
    "model_victim = setup_victim_model(\n",
    "    config[\"DEFAULT\"][\"model_architecture\"],\n",
    "    victim_path,\n",
    "    int(config[\"DEFAULT\"][\"number_of_classes\"]))\n",
    "\n",
    "device_string = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_string)\n",
    "log.info(\"Using device: {}\".format(device_string))\n",
    "\n",
    "model_victim = model_victim.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:49.109425Z",
     "start_time": "2020-08-26T08:48:47.332014Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/1347718638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  Load test set and transform it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_set, test_set = download_data(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DEFAULT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DEFAULT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_save_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DEFAULT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_data' is not defined"
     ]
    }
   ],
   "source": [
    "#  Load test set and transform it.\n",
    "train_set, test_set = download_data(\n",
    "    config[\"DEFAULT\"][\"dataset_name\"],\n",
    "    config[\"DEFAULT\"][\"test_save_path\"],\n",
    "    int(config[\"DEFAULT\"][\"input_size\"])\n",
    ")\n",
    "\n",
    "batch_size = config[\"DEFAULT\"][\"batch_size\"]\n",
    "train_set = data.DataLoader(train_set, batch_size=int(batch_size), shuffle=True, num_workers=4)\n",
    "test_set = data.DataLoader(test_set, batch_size=int(batch_size), shuffle=False, num_workers=4)\n",
    "_, _ = test_model(model_victim, test_set, int(config[\"DEFAULT\"][\"number_of_classes\"]))\n",
    "\n",
    "#  Determine size of the watermark filter\n",
    "watermark_shape, watermark_latent_shapes = get_shapes(model_victim, test_set)\n",
    "log.info(\"Input shape: {}\".format(watermark_shape))\n",
    "for latent_shape in watermark_latent_shapes:\n",
    "    log.info(\"Latent shape: {}\".format(latent_shape))\n",
    "\n",
    "key = watermark_filter.default_key(256)\n",
    "wf = watermark_filter.WatermarkFilter(key, watermark_shape, precision=16, probability=(5/1000))\n",
    "wf_latents = [\n",
    "    watermark_filter.WatermarkFilter(key, latent_shape, precision=16, probability=(50/1000))\n",
    "    for latent_shape in watermark_latent_shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:50.226251Z",
     "start_time": "2020-08-26T08:48:49.110603Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compare_distributions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/503010174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compare the distribution in the input space (image) to distribution of the latent representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_victim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf_latents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'compare_distributions' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare the distribution in the input space (image) to distribution of the latent representation\n",
    "lat = compare_distributions(model_victim, test_set, wf, wf_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:50.285484Z",
     "start_time": "2020-08-26T08:48:50.228523Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/3479471117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlat_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlist_of_batches\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lat' is not defined"
     ]
    }
   ],
   "source": [
    "def flatten(list_of_batches):\n",
    "    flat = []\n",
    "    for batch in list_of_batches:\n",
    "        for x in batch:\n",
    "            flat.append(x)\n",
    "    return flat\n",
    "            \n",
    "lat_flat = [flatten(list_of_batches) for list_of_batches in lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:55.148907Z",
     "start_time": "2020-08-26T08:48:50.287494Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lat_flat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/1079460949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlatent_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlat_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_flat_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msingle_flat_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlat_flat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lat_flat' is not defined"
     ]
    }
   ],
   "source": [
    "def create_dist(latent_flat):\n",
    "    l = latent_flat[0].shape[0]\n",
    "    latent_dist = [[] for _ in range(l)]\n",
    "    \n",
    "    for single_lat in tqdm(latent_flat):\n",
    "        for i in range(l):\n",
    "            latent_dist[i].append(single_lat[i])\n",
    "        \n",
    "    return latent_dist\n",
    "\n",
    "lat_dists = [create_dist(single_flat_list) for single_flat_list in lat_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:59.550926Z",
     "start_time": "2020-08-26T08:48:55.150222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'watermark_latent_shapes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/3188278022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmedians_for_lat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat_dist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwatermark_latent_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmedians_for_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'watermark_latent_shapes' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate medians that are then used to partition the latent space.\n",
    "\n",
    "medians_for_lat = []\n",
    "for shape, lat_dist in zip(watermark_latent_shapes, lat_dists):\n",
    "    medians_for_single = []\n",
    "    \n",
    "    for dist in lat_dist:\n",
    "        d = np.asarray(dist)\n",
    "        median = np.median(d)\n",
    "        medians_for_single.append(median)\n",
    "#         Optional plotting\n",
    "#         plt.hist(d)\n",
    "#         plt.show()\n",
    "        \n",
    "    medians_for_lat.append(medians_for_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T08:48:59.571776Z",
     "start_time": "2020-08-26T08:48:59.552447Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'watermark_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/2441828425.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mwf_latent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwatermark_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWatermarkFilter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmedians\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlat_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'watermark_filter' is not defined"
     ]
    }
   ],
   "source": [
    "def median_featurize(tensor_vector, medians):\n",
    "    for idx, v in enumerate(medians):\n",
    "        tensor_vector[idx] = 0 if tensor_vector[idx] <= 0 else 1\n",
    "    \n",
    "    return tensor_vector\n",
    "\n",
    "def do_mapping(\n",
    "    model: nn.Module,\n",
    "    test_set: data.DataLoader,\n",
    "    wf_latent: watermark_filter.WatermarkFilter,\n",
    "    medians: List,\n",
    "    lat_idx,\n",
    "    eps_test):\n",
    "\n",
    "    matching = 0\n",
    "    not_matching = 0\n",
    "    matching_and_same_label = 0\n",
    "    matching_and_diff_label = 0\n",
    "    not_matching_and_same_label = 0\n",
    "    not_matching_and_diff_label = 0\n",
    "    to_wm_cnt = 0\n",
    "\n",
    "    new_img_per_orig = 10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, _) in tqdm(test_set, unit=\"images\", desc=\"Watermark Filter\", leave=True, ascii=True):\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            ypred, latents = model(inputs)\n",
    "            _, predicted = torch.max(ypred.data, 1)\n",
    "            lats = latents[idx]\n",
    "                \n",
    "            for x, l, yp in zip(inputs, lats, predicted):\n",
    "                perturbed = perturb(x, eps_test)\n",
    "\n",
    "                assert len(l.shape) == 1\n",
    "                to_wm = wf_latent.is_watermark(median_featurize(l.cpu(), medians))\n",
    "\n",
    "                if to_wm:\n",
    "                    to_wm_cnt += 1\n",
    "\n",
    "                for _ in range(new_img_per_orig):\n",
    "                    input_star = perturb(x, eps_test)\n",
    "\n",
    "                    ypred_star, lat_star = model(input_star.unsqueeze(0))\n",
    "                    _, predicted_star = torch.max(ypred_star.data, 1)\n",
    "                    predicted_star = predicted_star.squeeze()\n",
    "\n",
    "                    lat_star = lat_star[idx].squeeze(0)\n",
    "                    assert len(lat_star.shape) == 1\n",
    "                    to_wm_star = wf_latent.is_watermark(median_featurize(lat_star.cpu(), medians))\n",
    "\n",
    "                    if to_wm_star == to_wm:\n",
    "                        matching += 1\n",
    "                        if yp == predicted_star:\n",
    "                            matching_and_same_label += 1\n",
    "                        else:\n",
    "                            matching_and_diff_label += 1\n",
    "                    else:\n",
    "                        not_matching += 1\n",
    "                        if yp == predicted_star:\n",
    "                            not_matching_and_same_label += 1\n",
    "                        else:\n",
    "                            not_matching_and_diff_label += 1\n",
    "\n",
    "    log.info(\"to wm: {}\".format(to_wm_cnt))\n",
    "    log.info(\"matching: {} same label {} diff label {}\".format(\n",
    "        matching, matching_and_same_label, matching_and_diff_label))\n",
    "    log.info(\"not matching: {} same label {} diff label {}\".format(\n",
    "        not_matching, not_matching_and_same_label, not_matching_and_diff_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T09:25:42.922463Z",
     "start_time": "2020-08-26T08:48:59.572705Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "+++ with eps: 0.2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wf_latents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2285/2283822112.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+++ with eps: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf_latents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmedians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedians_for_lat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nlatent size: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedians\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wf_latents' is not defined"
     ]
    }
   ],
   "source": [
    "for eps in [0.2, 0.1, 0.09, 0.075, 0.05]:\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"+++ with eps: {}\".format(eps))\n",
    "    for idx, wf in enumerate(wf_latents):\n",
    "        medians = medians_for_lat[idx]\n",
    "        print(\"\\nlatent size: {}\".format(len(medians)))\n",
    "        do_mapping(\n",
    "            model_victim,\n",
    "            test_set,\n",
    "            wf,\n",
    "            medians,\n",
    "            idx,\n",
    "            eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
